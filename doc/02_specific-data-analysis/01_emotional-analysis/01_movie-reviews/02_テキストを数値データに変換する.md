# 2. テキストを数値データに変換する

# もくじ
- [2. テキストを数値データに変換する](#2-テキストを数値データに変換する)
- [もくじ](#もくじ)
- [1. BoWモデルを利用してテキストを数値に変換する](#1-bowモデルを利用してテキストを数値に変換する)
  - [1.1. BoWモデルとは](#11-bowモデルとは)
  - [1.2. 特徴量ベクトルを作る](#12-特徴量ベクトルを作る)
  - [1.3. BoWモデルの問題点](#13-bowモデルの問題点)
- [2. TF-IDFで単語の関連性を評価する](#2-tf-idfで単語の関連性を評価する)
  - [2.1. TF-IDFとは](#21-tf-idfとは)
- [99. 参考](#99-参考)

# 1. BoWモデルを利用してテキストを数値に変換する

## 1.1. BoWモデルとは

文章や単語などのカテゴリデータは、機械学習アルゴリズムに渡す前に数値に変換しておく必要があります。ここではテキストを数値の特徴量ベクトルとして表現できる **BoW（Bag-of-Words）モデル**を紹介します。

BoWモデルは以下の考えに則っているものです。

1. 文章全体から単語（token）からなる語彙（vocabulary）を作成する。
    1. 一意に決まることを意味してtokenと定めています。
2. 各文章での単語の出現回数を含んだ特徴ベクトルを生成する。

例えば「私は犬が好きです」「彼は犬が嫌いです」という二つの文章には「私」「彼」「好き」「嫌い」という単語はどちらか一つの文章にしか出現せず、「犬」「は」「が」「です」はどちらの文章にも登場しています。

$['私', '彼', 'は', '犬', 'が', '好き', '嫌い', 'です']]$のそれぞれの単語が、ある文章について何回出現するかを数えると、

$[[1, 0, 1, 1, 1, 1, 0, 1], [0, 1, 1, 1, 1, 0, 1, 1]]$

となります。

この特徴ベクトルにおける値は単語の**生の出現度**と考えられ、$tf(t, d)$と表されます。

（これは後で**TF-IDF**というモデルを考えるときにまた出てきます。）

また今回は短いかつ似たような単語で構成された文章なので0,1がバランス良く出ていますが、基本的には一つ一つの文章が長く、かつたくさんの文章が存在すれば単語の生の出現度は低くなり、ベクトルのほとんどが0となるため、この特徴ベクトルは**「疎ベクトル」**とも呼ばれます。

## 1.2. 特徴量ベクトルを作る

scikit-learnに搭載されているCountVectorizerを利用すると以下のようにBoWモデルを構築できます。

```python
#!/usr/bin/env python

import numpy as np
import MeCab
from sklearn.feature_extraction.text import CountVectorizer

# MeCab による単語への分割関数 (名詞のみ残す)
def split_text_only_noun(text):
    tagger = MeCab.Tagger()

    words = []
    for c in tagger.parse(text).splitlines()[:-1]:
        surface, feature = c.split('\t')
        pos = feature.split(',')[0]
        if pos == '名詞':
            words.append(surface)
   return ' '.join(words)

messages_list = [
    split_text_only_noun('私達はラーメンがとても大好きです。'),
    split_text_only_noun('私達は蕎麦がとても大好きです。')
]
docs = np.array(messages_list)
# print(messages_list)
# ['私達 ラーメン 大好き', '私達 蕎麦 大好き']

# bow ( bag of words )
count = CountVectorizer()
bags = count.fit_transform(docs)

print(bags.toarray())
# [[1 1 1 0]
#  [0 1 1 1]]

features = count.get_feature_names()
print(features)
# ['ラーメン', '大好き', '私達', '蕎麦']
```

最初の多次元配列（`[1 1 1 0]`）がこの特徴ベクトルであり、次のディクショナリ（`[0 1 1 1]`）をみるとそれぞれの単語がどれだけ出現しているかを見ることができます。

## 1.3. BoWモデルの問題点

BOWの問題として単純な単語の計数としているため、その単語が文書集合の中でどのくらい重要であるのか、といった情報が欠録しています。そのため単純に文書が長いほうが有利になると行った問題点があります。

そこで文書集合全体をみて重みをつける**TF-IDF**が考案されました。

# 2. TF-IDFで単語の関連性を評価する

## 2.1. TF-IDFとは

TF-IDFは、TF(Term Frequency)とIDF(Inverse Document Frequency)を組み合わせたものです。

**TF(Term Frequency)**

各文書においてその単語がどのくらい出現したのかを表します。その文書内でたくさん出ている単語ほど重要としています。

**IDF(Inverse Document Frequency)**

全文書内で単語の出現があまりない場合は高い値、様々な文書に出現する単語なら低い値とし、様々な文書で横断的に使われている単語は重要な単語ではないとします。

これらを掛け合わせたものをTF-IDFとします。

すなわち、文書の中で頻度の高い単語と文書集合の中で頻度が低い単語に重みをつける手法です。

```python
import numpy as np

from sklearn.feature_extraction.text import TfidfTransformer

# tf-idf
tfidf = TfidfTransformer(use_idf=True, norm='l2', smooth_idf=True)
np.set_printoptions(precision=2)
tf_idf = tfidf.fit_transform(bags)

print(tf_idf.toarray())

# [[0.7 0.5 0.5 0. ]
#  [0.  0.5 0.5 0.7]]
```

以下のようにそれぞれの単語が数から少数の重み付けの値へと変化します。

```python
[[1 1 1 0]
 [0 1 1 1]]

[[0.7 0.5 0.5 0. ]
 [0.  0.5 0.5 0.7]]

['ラーメン', '大好き', '私達', '蕎麦']
```

最初の文字列の場合は、'大好き'と'私達'がどちらの文章にも現れているため、0.5と値は低めになっています。その代わり、'ラーメン'と'蕎麦'が片方しか出ていない単語のため、0.7と重み付けの値が少し高めになっています。

# 99. 参考

- [Python3を使った日本語自然言語処理③疎ベクトルの生成 - Qiita](https://qiita.com/asai0304/items/1ab973f18925c9d718d5)

- 実装面での参考
  - [自然言語処理の基礎である形態素解析からbowによるベクトル化、TF-IDFによる重み付けまで解説｜shimakaze_soft｜note](https://note.com/shimakaze_soft/n/nf02b0f8ab0f6#Tt0zA)

- [自然言語処理を理解するためにベクトル化手法について学ぶ(BOW, TF-IDF, Word2Vec) - Qiita](https://qiita.com/abe_yu/items/b15ce4738906f96ee987)

- BoW以外の手法
  - [自然言語処理の概要まとめ - Qiita](https://qiita.com/Hiroki1928/items/ebdf9fe1201e26917b72)